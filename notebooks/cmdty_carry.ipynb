{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c28eec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import options_wizard as ow\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "tick_path = os.getenv(\"TICK_PATH\", \"\").split(os.pathsep)[0]\n",
    "\n",
    "p = Path(tick_path)\n",
    "if p.is_dir(): \n",
    "    available_ticks =[f.name.replace('.parquet', '') for f in p.iterdir() if f.is_file()]\n",
    "\n",
    "n_const = 50\n",
    "\n",
    "suffix = f\"50_delta_7_1_cal_spread\"\n",
    "start_date = ow.DateObj(2010, 1, 1)\n",
    "end_date = ow.DateObj(2020, 12, 31)\n",
    "save_type = ow.SaveType.PICKLE\n",
    "\n",
    "universe = ow.Universe()\n",
    "universe.top_constituents(n_const)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cfe1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Setup\n",
    "# --------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "cmdty_path = os.getenv(\"CMDTY_ROLL_PATH\", \"\").split(os.pathsep)[0]\n",
    "cmdty_folders = [f.path for f in os.scandir(cmdty_path) if f.is_dir()]\n",
    "\n",
    "\n",
    "for samp in cmdty_folders:\n",
    "    files = sorted(\n",
    "        f.path for f in os.scandir(samp)\n",
    "        if f.is_file() and f.name.endswith(\".parquet\")\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Load all parquet files and stack with Contract\n",
    "    # --------------------------------------------------\n",
    "    dfs = []\n",
    "\n",
    "    for f in files:\n",
    "\n",
    "        df = pd.read_parquet(f)\n",
    "\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "        df = df.drop_duplicates(subset=[\"Date\"])\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    merged = (\n",
    "        pd.concat(dfs, ignore_index=True)\n",
    "        .set_index([\"Date\", \"Contract\"])\n",
    "        .sort_index()\n",
    "    )\n",
    "    merged = merged.dropna(subset=[\"PX_SETTLE\", \"LAST_TRADEABLE_DT\"])\n",
    "\n",
    "    merged = (\n",
    "        merged\n",
    "        .reset_index()          # Date, Contract → columns\n",
    "        .set_index(\"Date\")      # Date → index\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    # 3. Save merged DataFrame\n",
    "    # --------------------------------------------------\n",
    "    save_path = os.path.join(os.getenv(\"CMDTY_PATH\"), samp.split('\\\\')[-1] + \".parquet\")\n",
    "    merged[\"FUT_CONT_SIZE\"]= pd.to_numeric(merged[\"FUT_CONT_SIZE\"], errors='coerce')\n",
    "    merged[\"CRNCY\"] = merged[\"CRNCY\"].astype(\"string\")\n",
    "    merged[\"EXCH_CODE\"] = merged[\"EXCH_CODE\"].astype(\"string\")\n",
    "\n",
    "    merged.to_parquet(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "42e9784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import options_wizard as ow\n",
    "import polars as pl\n",
    "\n",
    "load_dotenv()\n",
    "cmdty_path = os.getenv(\"CMDTY_PATH\", \"\").split(os.pathsep)[0]\n",
    "files = [f.path for f in os.scandir(cmdty_path) if f.is_file() and f.name.endswith(\".parquet\")]\n",
    "\n",
    "for f in files:\n",
    "    df = pl.scan_parquet(f)\n",
    "    break\n",
    "\n",
    "df.collect().head(5)\n",
    "\n",
    "\n",
    "def load_data(**kwargs) -> ow.DataType:\n",
    "    \"\"\"Loads in the commodity futures data\"\"\"\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    import polars as pl\n",
    "\n",
    "    tick = kwargs.get(\"tick\", None)\n",
    "\n",
    "    load_dotenv()\n",
    "    cmdty_path = os.getenv(\"CMDTY_PATH\", \"\").split(os.pathsep)[0]\n",
    "    files = [f.path for f in os.scandir(cmdty_path) if f.is_file() and f.name.endswith(\".parquet\")]\n",
    "\n",
    "    df = None\n",
    "    for file in files:\n",
    "        if tick == file.split('\\\\')[-1].replace('_FUT.parquet', ''):\n",
    "            df = pl.scan_parquet(file)\n",
    "    \n",
    "    if df is None:\n",
    "        raise ValueError(f\"Tick {tick} not found in CMDTY_PATH\")\n",
    "    \n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .with_columns(\n",
    "        pl.col(\"LAST_TRADEABLE_DT\")\n",
    "        .str.strptime(pl.Date, format=\"%d/%m/%Y\", strict=False)\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(\"FUT_NOTICE_FIRST\")\n",
    "            .str.strptime(pl.Date, format=\"%d/%m/%Y\", strict=False)\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(\"Date\")\n",
    "            .cast(pl.Date)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return ow.DataType(data = df, tick = tick)\n",
    "\n",
    "def days_to_anchor(data: ow.DataType, **kwargs) -> ow.DataType:\n",
    "\n",
    "    import pandas as pd\n",
    "    import polars as pl\n",
    "    import options_wizard as ow\n",
    "\n",
    "    df = data._data\n",
    "    tick = kwargs.get(\"tick\", None)\n",
    "\n",
    "    min_day = pd.Timestamp(\n",
    "        df.select(pl.col(\"Date\").min()).collect().item()\n",
    "    )\n",
    "\n",
    "    max_day = pd.Timestamp(\n",
    "        df.select(pl.col(\"LAST_TRADEABLE_DT\").max()).collect().item()\n",
    "    )\n",
    "\n",
    "    exchange = (\n",
    "        df.select(pl.col(\"EXCH_CODE\"))\n",
    "        .unique()\n",
    "        .collect()\n",
    "        .item()\n",
    "    )\n",
    "\n",
    "    dates = ow.market_dates(\n",
    "        exchange=exchange,\n",
    "        lower=min_day,\n",
    "        upper=max_day\n",
    "    )\n",
    "\n",
    "    calendar = (\n",
    "        pl.LazyFrame({\"Date\": dates})\n",
    "        .with_columns(pl.col(\"Date\").cast(pl.Date))\n",
    "        .with_row_index(\"TradeDateIdx\")\n",
    "        .sort(\"Date\")               \n",
    "    )\n",
    "\n",
    "    df = df.sort(\"Date\")\n",
    "\n",
    "    df = (\n",
    "        df.join_asof(\n",
    "            calendar,\n",
    "            on=\"Date\",\n",
    "            strategy=\"backward\"\n",
    "        )\n",
    "        .filter(pl.col(\"TradeDateIdx\").is_not_null())\n",
    "        .rename({\"TradeDateIdx\": \"DateIdx\"})\n",
    "    )\n",
    "\n",
    "    calendar_ltd = (\n",
    "        calendar\n",
    "        .rename({\n",
    "            \"Date\": \"LAST_TRADEABLE_DT\",\n",
    "            \"TradeDateIdx\": \"LtdIdx\"\n",
    "        })\n",
    "        .sort(\"LAST_TRADEABLE_DT\")   \n",
    "    )\n",
    "\n",
    "    df = df.sort(\"LAST_TRADEABLE_DT\")\n",
    "\n",
    "    df = (\n",
    "        df.join_asof(\n",
    "            calendar_ltd,\n",
    "            on=\"LAST_TRADEABLE_DT\",\n",
    "            strategy=\"backward\"\n",
    "        )\n",
    "        .filter(pl.col(\"LtdIdx\").is_not_null())\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        (pl.col(\"LtdIdx\") - pl.col(\"DateIdx\")).alias(\"DAYS_TO_ANCHOR\")\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"DAYS_TO_ANCHOR\")\n",
    "        .min()\n",
    "        .over(\"Date\")\n",
    "        .alias(\"DAYS_TO_FRONT_ANCHOR\")\n",
    "    )\n",
    "    return ow.DataType(data = df, tick =tick )\n",
    "\n",
    "def curve_structure(data: ow.DataType, **kwargs) -> ow.DataType:\n",
    "    import polars as pl\n",
    "    import options_wizard as ow\n",
    "\n",
    "    df = data._data\n",
    "    tick = kwargs.get(\"tick\", None)\n",
    "\n",
    "    # --- build clean cross-sectional curve (one price per tenor per day) ---\n",
    "    curve = (\n",
    "        df\n",
    "        .select([\"Date\", \"DAYS_TO_ANCHOR\", \"PX_SETTLE\"])\n",
    "        .group_by([\"Date\", \"DAYS_TO_ANCHOR\"])\n",
    "        .agg(pl.col(\"PX_SETTLE\").last())\n",
    "        .sort([\"Date\", \"DAYS_TO_ANCHOR\"])\n",
    "        .with_columns(\n",
    "            pl.row_index().over(\"Date\").alias(\"TenorIdx\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- identify front / back / interior ---\n",
    "    curve = curve.with_columns([\n",
    "        pl.when(pl.col(\"TenorIdx\") == 0)\n",
    "          .then(pl.lit(\"front\"))\n",
    "          .when(pl.col(\"TenorIdx\") == pl.max(\"TenorIdx\").over(\"Date\"))\n",
    "          .then(pl.lit(\"back\"))\n",
    "          .otherwise(pl.lit(\"int\"))\n",
    "          .alias(\"CURVE_POS\")\n",
    "    ])\n",
    "\n",
    "    # --- neighbour tenors in tenor space ---\n",
    "    curve = curve.with_columns([\n",
    "        pl.col(\"PX_SETTLE\").shift(-1).over(\"Date\").alias(\"f_p\"),\n",
    "        pl.col(\"PX_SETTLE\").shift(1).over(\"Date\").alias(\"f_m\"),\n",
    "        pl.col(\"DAYS_TO_ANCHOR\").shift(-1).over(\"Date\").alias(\"T_p\"),\n",
    "        pl.col(\"DAYS_TO_ANCHOR\").shift(1).over(\"Date\").alias(\"T_m\"),\n",
    "    ])\n",
    "\n",
    "    # --- weighted second derivative (cross-sectional curvature) ---\n",
    "    curve = curve.with_columns([\n",
    "        (pl.col(\"T_p\") - pl.col(\"DAYS_TO_ANCHOR\")).alias(\"dt_fwd\"),\n",
    "        (pl.col(\"DAYS_TO_ANCHOR\") - pl.col(\"T_m\")).alias(\"dt_bwd\"),\n",
    "        (pl.col(\"T_p\") - pl.col(\"T_m\")).alias(\"dt_span\"),\n",
    "    ]).with_columns(\n",
    "        pl.when(\n",
    "            pl.all_horizontal([\n",
    "                pl.col(\"f_p\").is_not_null(),\n",
    "                pl.col(\"f_m\").is_not_null(),\n",
    "                pl.col(\"dt_fwd\") > 0,\n",
    "                pl.col(\"dt_bwd\") > 0,\n",
    "            ])\n",
    "        )\n",
    "        .then(\n",
    "            2.0 / pl.col(\"dt_span\") * (\n",
    "                (pl.col(\"f_p\") - pl.col(\"PX_SETTLE\")) / pl.col(\"dt_fwd\")\n",
    "                -\n",
    "                (pl.col(\"PX_SETTLE\") - pl.col(\"f_m\")) / pl.col(\"dt_bwd\")\n",
    "            ) * (252.0 ** 2)\n",
    "        )\n",
    "        .otherwise(None)\n",
    "        .alias(\"CURVATURE\")\n",
    "    )\n",
    "\n",
    "    # --- relative curvature (scale-free) ---\n",
    "    curve = curve.with_columns(\n",
    "        (pl.col(\"CURVATURE\") / pl.col(\"PX_SETTLE\")).alias(\"REL_CURVATURE\")\n",
    "    )\n",
    "\n",
    "    # --- keep only what we want to merge back ---\n",
    "    curve = curve.select([\n",
    "        \"Date\",\n",
    "        \"DAYS_TO_ANCHOR\",\n",
    "        \"CURVE_POS\",\n",
    "        \"REL_CURVATURE\",\n",
    "    ])\n",
    "\n",
    "    # --- merge back to original dataframe ---\n",
    "    out = df.join(\n",
    "        curve,\n",
    "        on=[\"Date\", \"DAYS_TO_ANCHOR\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return ow.DataType(data=out, tick=tick)\n",
    "\n",
    "\n",
    "data = load_data(tick = \"CC\")\n",
    "data = days_to_anchor(data, tick = \"CC\")\n",
    "data = curve_structure(data, tick = \"CC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4fa19b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>value</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>17897.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>0.01201</td></tr><tr><td>&quot;std&quot;</td><td>0.309006</td></tr><tr><td>&quot;min&quot;</td><td>-2.93395</td></tr><tr><td>&quot;25%&quot;</td><td>-0.077484</td></tr><tr><td>&quot;50%&quot;</td><td>-0.017291</td></tr><tr><td>&quot;75%&quot;</td><td>0.038237</td></tr><tr><td>&quot;max&quot;</td><td>4.588169</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 2)\n",
       "┌────────────┬───────────┐\n",
       "│ statistic  ┆ value     │\n",
       "│ ---        ┆ ---       │\n",
       "│ str        ┆ f64       │\n",
       "╞════════════╪═══════════╡\n",
       "│ count      ┆ 17897.0   │\n",
       "│ null_count ┆ 0.0       │\n",
       "│ mean       ┆ 0.01201   │\n",
       "│ std        ┆ 0.309006  │\n",
       "│ min        ┆ -2.93395  │\n",
       "│ 25%        ┆ -0.077484 │\n",
       "│ 50%        ┆ -0.017291 │\n",
       "│ 75%        ┆ 0.038237  │\n",
       "│ max        ┆ 4.588169  │\n",
       "└────────────┴───────────┘"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = data._data.collect().filter(pl.col(\"CURVE_POS\") == \"int\")\n",
    "out['REL_CURVATURE'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "681320ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ColumnNotFoundError",
     "evalue": "unable to find column \"DAYS_TO_ANCHOR\"; valid columns: [\"Date\", \"CURV_TENOR_CURV_TENOR_1\", \"CURV_TENOR_CURV_TENOR_2\", \"CURV_TENOR_CURV_TENOR_3\", \"CURV_TENOR_CURV_TENOR_4\"]\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'sink' <---\nSELECT [col(\"Date\"), col(\"CURV_TENOR_1\").alias(\"CURV_TENOR_CURV_TENOR_1\"), col(\"CURV_TENOR_2\").alias(\"CURV_TENOR_CURV_TENOR_2\"), col(\"CURV_TENOR_3\").alias(\"CURV_TENOR_CURV_TENOR_3\"), col(\"CURV_TENOR_4\").alias(\"CURV_TENOR_CURV_TENOR_4\")]\n  SORT BY [col(\"Date\")]\n    AGGREGATE[maintain_order: false]\n      [when([(col(\"TenorIdx\")) == (1)]).then(col(\"CURVATURE\")).otherwise(null.cast(Float64)).first().alias(\"CURV_TENOR_1\"), when([(col(\"TenorIdx\")) == (2)]).then(col(\"CURVATURE\")).otherwise(null.cast(Float64)).first().alias(\"CURV_TENOR_2\"), when([(col(\"TenorIdx\")) == (3)]).then(col(\"CURVATURE\")).otherwise(null.cast(Float64)).first().alias(\"CURV_TENOR_3\"), when([(col(\"TenorIdx\")) == (4)]).then(col(\"CURVATURE\")).otherwise(null.cast(Float64)).first().alias(\"CURV_TENOR_4\")] BY [col(\"Date\")]\n      FROM\n      SELECT [col(\"Date\"), col(\"TenorIdx\"), col(\"CURVATURE\")]\n        FILTER col(\"CURVATURE\").is_not_null()\n        FROM\n           WITH_COLUMNS:\n           [[([(dyn float: 2) / ([(col(\"T_p\")) - (col(\"T_m\"))].cast(Unknown(Float)))]) * ([([([(col(\"F_p\")) - (col(\"MID_PX\"))]) / ([(col(\"T_p\")) - (col(\"DAYS_TO_ANCHOR\"))].cast(Float64))]) - ([([(col(\"MID_PX\")) - (col(\"F_m\"))]) / ([(col(\"DAYS_TO_ANCHOR\")) - (col(\"T_m\"))].cast(Float64))])])].alias(\"CURVATURE\")] \n             WITH_COLUMNS:\n             [col(\"MID_PX\").shift([dyn int: -1]).over([col(\"Date\")]).alias(\"F_p\"), col(\"MID_PX\").shift([dyn int: 1]).over([col(\"Date\")]).alias(\"F_m\"), col(\"DAYS_TO_ANCHOR\").shift([dyn int: -1]).over([col(\"Date\")]).alias(\"T_p\"), col(\"DAYS_TO_ANCHOR\").shift([dyn int: 1]).over([col(\"Date\")]).alias(\"T_m\")] \n               WITH_COLUMNS:\n               [0.int_range([len()]).over([col(\"Date\")]).alias(\"TenorIdx\")] \n                SORT BY [col(\"Date\"), col(\"DAYS_TO_ANCHOR\")]\n                   WITH_COLUMNS:\n                   [[([(col(\"PX_BID\")) + (col(\"PX_ASK\"))]) / (2.0)].alias(\"MID_PX\")] \n                     WITH_COLUMNS:\n                     [col(\"DAYS_TO_ANCHOR\").min().over([col(\"Date\")]).alias(\"DAYS_TO_FRONT_ANCHOR\")] \n                       WITH_COLUMNS:\n                       [[(col(\"LtdIdx\")) - (col(\"DateIdx\"))].alias(\"DAYS_TO_ANCHOR\")] \n                        FILTER col(\"LtdIdx\").is_not_null()\n                        FROM\n                          ASOF JOIN:\n                          LEFT PLAN ON: [col(\"LAST_TRADEABLE_DT\")]\n                            SORT BY [col(\"LAST_TRADEABLE_DT\")]\n                              SELECT [col(\"Contract\"), col(\"VOLUME\"), col(\"OPEN_INT\"), col(\"PX_BID\"), col(\"PX_ASK\"), col(\"PX_SETTLE\"), col(\"LAST_TRADEABLE_DT\"), col(\"FUT_NOTICE_FIRST\"), col(\"FUT_CONT_SIZE\"), col(\"CRNCY\"), col(\"EXCH_CODE\"), col(\"Date\"), col(\"TradeDateIdx\").alias(\"DateIdx\")]\n                                FILTER col(\"TradeDateIdx\").is_not_null()\n                                FROM\n                                  ASOF JOIN:\n                                  LEFT PLAN ON: [col(\"Date\")]\n                                    SORT BY [col(\"Date\")]\n                                       WITH_COLUMNS:\n                                       [col(\"Date\").strict_cast(Date)] \n                                         WITH_COLUMNS:\n                                         [col(\"FUT_NOTICE_FIRST\").str.strptime([\"raise\"])] \n                                           WITH_COLUMNS:\n                                           [col(\"LAST_TRADEABLE_DT\").str.strptime([\"raise\"])] \n                                            Parquet SCAN [S:\\\\Trading\\\\NSA\\\\Christoph\\\\Commodity Futures\\\\updated\\CC_FUT.parquet]\n                                            PROJECT */12 COLUMNS\n                                            ESTIMATED ROWS: 26881\n                                  RIGHT PLAN ON: [col(\"Date\")]\n                                    SORT BY [col(\"Date\")]\n                                      ROW INDEX name: TradeDateIdx, offset: 0\n                                         WITH_COLUMNS:\n                                         [col(\"Date\").strict_cast(Date)] \n                                          DF [\"Date\"]; PROJECT */1 COLUMNS\n                                  END ASOF JOIN\n                          RIGHT PLAN ON: [col(\"LAST_TRADEABLE_DT\")]\n                            SORT BY [col(\"LAST_TRADEABLE_DT\")]\n                              SELECT [col(\"TradeDateIdx\").alias(\"LtdIdx\"), col(\"Date\").alias(\"LAST_TRADEABLE_DT\")]\n                                SORT BY [col(\"Date\")]\n                                  ROW INDEX name: TradeDateIdx, offset: 0\n                                     WITH_COLUMNS:\n                                     [col(\"Date\").strict_cast(Date)] \n                                      DF [\"Date\"]; PROJECT */1 COLUMNS\n                          END ASOF JOIN",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mColumnNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpl\u001b[39;00m\n\u001b[32m      3\u001b[39m lf = (\n\u001b[32m      4\u001b[39m     data._data.group_by(\u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m).agg(pl.col(\u001b[33m\"\u001b[39m\u001b[33mDAYS_TO_ANCHOR\u001b[39m\u001b[33m\"\u001b[39m).implode().alias(\u001b[33m\"\u001b[39m\u001b[33mdays_to_anchor\u001b[39m\u001b[33m\"\u001b[39m))  \u001b[38;5;66;03m# collect into list per date.sort(\"Date\")  # optional\u001b[39;00m\n\u001b[32m      5\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df = \u001b[43mlf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Print each date followed by the DAYS_TO_ANCHOR values for its contracts\u001b[39;00m\n\u001b[32m     10\u001b[39m df = df.sort(\u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cameronA\\AppData\\Local\\miniconda3\\envs\\etfs\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cameronA\\AppData\\Local\\miniconda3\\envs\\etfs\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:328\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    327\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cameronA\\AppData\\Local\\miniconda3\\envs\\etfs\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2429\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2427\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2428\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mColumnNotFoundError\u001b[39m: unable to find column \"DAYS_TO_ANCHOR\"; valid columns: [\"Date\", \"CURV_TENOR_CURV_TENOR_1\", \"CURV_TENOR_CURV_TENOR_2\", \"CURV_TENOR_CURV_TENOR_3\", \"CURV_TENOR_CURV_TENOR_4\"]\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'sink' <---\nSELECT [col(\"Date\"), col(\"CURV_TENOR_1\").alias(\"CURV_TENOR_CURV_TENOR_1\"), col(\"CURV_TENOR_2\").alias(\"CURV_TENOR_CURV_TENOR_2\"), col(\"CURV_TENOR_3\").alias(\"CURV_TENOR_CURV_TENOR_3\"), col(\"CURV_TENOR_4\").alias(\"CURV_TENOR_CURV_TENOR_4\")]\n  SORT BY [col(\"Date\")]\n    AGGREGATE[maintain_order: false]\n      [when([(col(\"TenorIdx\")) == (1)]).then(col(\"CURVATURE\")).otherwise(null.cast(Float64)).first().alias(\"CURV_TENOR_1\"), when([(col(\"TenorIdx\")) == (2)]).then(col(\"CURVATURE\")).otherwise(null.cast(Float64)).first().alias(\"CURV_TENOR_2\"), when([(col(\"TenorIdx\")) == (3)]).then(col(\"CURVATURE\")).otherwise(null.cast(Float64)).first().alias(\"CURV_TENOR_3\"), when([(col(\"TenorIdx\")) == (4)]).then(col(\"CURVATURE\")).otherwise(null.cast(Float64)).first().alias(\"CURV_TENOR_4\")] BY [col(\"Date\")]\n      FROM\n      SELECT [col(\"Date\"), col(\"TenorIdx\"), col(\"CURVATURE\")]\n        FILTER col(\"CURVATURE\").is_not_null()\n        FROM\n           WITH_COLUMNS:\n           [[([(dyn float: 2) / ([(col(\"T_p\")) - (col(\"T_m\"))].cast(Unknown(Float)))]) * ([([([(col(\"F_p\")) - (col(\"MID_PX\"))]) / ([(col(\"T_p\")) - (col(\"DAYS_TO_ANCHOR\"))].cast(Float64))]) - ([([(col(\"MID_PX\")) - (col(\"F_m\"))]) / ([(col(\"DAYS_TO_ANCHOR\")) - (col(\"T_m\"))].cast(Float64))])])].alias(\"CURVATURE\")] \n             WITH_COLUMNS:\n             [col(\"MID_PX\").shift([dyn int: -1]).over([col(\"Date\")]).alias(\"F_p\"), col(\"MID_PX\").shift([dyn int: 1]).over([col(\"Date\")]).alias(\"F_m\"), col(\"DAYS_TO_ANCHOR\").shift([dyn int: -1]).over([col(\"Date\")]).alias(\"T_p\"), col(\"DAYS_TO_ANCHOR\").shift([dyn int: 1]).over([col(\"Date\")]).alias(\"T_m\")] \n               WITH_COLUMNS:\n               [0.int_range([len()]).over([col(\"Date\")]).alias(\"TenorIdx\")] \n                SORT BY [col(\"Date\"), col(\"DAYS_TO_ANCHOR\")]\n                   WITH_COLUMNS:\n                   [[([(col(\"PX_BID\")) + (col(\"PX_ASK\"))]) / (2.0)].alias(\"MID_PX\")] \n                     WITH_COLUMNS:\n                     [col(\"DAYS_TO_ANCHOR\").min().over([col(\"Date\")]).alias(\"DAYS_TO_FRONT_ANCHOR\")] \n                       WITH_COLUMNS:\n                       [[(col(\"LtdIdx\")) - (col(\"DateIdx\"))].alias(\"DAYS_TO_ANCHOR\")] \n                        FILTER col(\"LtdIdx\").is_not_null()\n                        FROM\n                          ASOF JOIN:\n                          LEFT PLAN ON: [col(\"LAST_TRADEABLE_DT\")]\n                            SORT BY [col(\"LAST_TRADEABLE_DT\")]\n                              SELECT [col(\"Contract\"), col(\"VOLUME\"), col(\"OPEN_INT\"), col(\"PX_BID\"), col(\"PX_ASK\"), col(\"PX_SETTLE\"), col(\"LAST_TRADEABLE_DT\"), col(\"FUT_NOTICE_FIRST\"), col(\"FUT_CONT_SIZE\"), col(\"CRNCY\"), col(\"EXCH_CODE\"), col(\"Date\"), col(\"TradeDateIdx\").alias(\"DateIdx\")]\n                                FILTER col(\"TradeDateIdx\").is_not_null()\n                                FROM\n                                  ASOF JOIN:\n                                  LEFT PLAN ON: [col(\"Date\")]\n                                    SORT BY [col(\"Date\")]\n                                       WITH_COLUMNS:\n                                       [col(\"Date\").strict_cast(Date)] \n                                         WITH_COLUMNS:\n                                         [col(\"FUT_NOTICE_FIRST\").str.strptime([\"raise\"])] \n                                           WITH_COLUMNS:\n                                           [col(\"LAST_TRADEABLE_DT\").str.strptime([\"raise\"])] \n                                            Parquet SCAN [S:\\\\Trading\\\\NSA\\\\Christoph\\\\Commodity Futures\\\\updated\\CC_FUT.parquet]\n                                            PROJECT */12 COLUMNS\n                                            ESTIMATED ROWS: 26881\n                                  RIGHT PLAN ON: [col(\"Date\")]\n                                    SORT BY [col(\"Date\")]\n                                      ROW INDEX name: TradeDateIdx, offset: 0\n                                         WITH_COLUMNS:\n                                         [col(\"Date\").strict_cast(Date)] \n                                          DF [\"Date\"]; PROJECT */1 COLUMNS\n                                  END ASOF JOIN\n                          RIGHT PLAN ON: [col(\"LAST_TRADEABLE_DT\")]\n                            SORT BY [col(\"LAST_TRADEABLE_DT\")]\n                              SELECT [col(\"TradeDateIdx\").alias(\"LtdIdx\"), col(\"Date\").alias(\"LAST_TRADEABLE_DT\")]\n                                SORT BY [col(\"Date\")]\n                                  ROW INDEX name: TradeDateIdx, offset: 0\n                                     WITH_COLUMNS:\n                                     [col(\"Date\").strict_cast(Date)] \n                                      DF [\"Date\"]; PROJECT */1 COLUMNS\n                          END ASOF JOIN"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "lf = (\n",
    "    data._data.group_by(\"Date\").agg(pl.col(\"DAYS_TO_ANCHOR\").implode().alias(\"days_to_anchor\"))  # collect into list per date.sort(\"Date\")  # optional\n",
    ")\n",
    "\n",
    "df = lf.collect()\n",
    "\n",
    "# Print each date followed by the DAYS_TO_ANCHOR values for its contracts\n",
    "df = df.sort(\"Date\")\n",
    "for date, values in df.iter_rows():\n",
    "    print(date, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day = in "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
